---
layout: 	post
title:  	"[Machine Learning] Principal Component Analysis"
date:   	2014-12-02 00:00:00-0000
modified:	2014-12-02 00:00:00-0000
categories: 
- Notes
tags:		[Notes, ML]
---

###Demensionality Reduction

![PCA][PCA]

In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration, and can be divided into feature selection and feature extraction.

In practice, demensionality reduction often be used for data compression and data visualization

###PCA

![projection][projection]

The intuition behind PCA is very simple. For example, to reduce 2-dimension data to 1-demension, we find a direction(a vector $$ \mu^{(1)}\subseteq\mathbb{R}^n $$) onto which to preject the data so as to minimize the projection error.To reduce n-demension to k-demension, we find k vectors $$ \mu^{(1)},\mu^{(2)},...,\mu^{(k)} $$.


To have a comparable range of value, when applying PCA, we should do the **mean normalization** which means subtract each sample x by the average of all sample at first.Then we compute the "covariance matrix" $$ \Sigma $$ by:

$$ \Sigma=\frac{1}{m}\sum_{i=1}^{n}(x^{(i)})(x^{(i)})^T $$

Then we compute "eigenvectors" of $$ \Sigma $$ as $$ U $$. Then new data in k-demension can be compute by:

$$ z=U(1:k)*x $$

###Chose K

When we apply PCA to data visualization, we just reduce the demension to two or three. But when we do data compression, what the proper K to chose? When we applying demension reduction, some variance of data is also reduced.To maintain the variance, we can measure the variance retained formula:

$$ \frac{\frac{1}{m}\sum_{i=1}^{m}\left\|x^{(i)}-x_{approx}^{(i)}\right\|^2}{\frac{1}{m}\sum_{i=1}^{m}\left\|x^{(i)}\right\|^2}<\epsilon $$

Typically, people take $$ \epsilon $$ as 0.01 to have 99% of variance retained. So we just chose k to be smallest value that satisfy the formula. 

[PCA]:/images/PCA.png
[projection]:/images/PCA_prejection.png